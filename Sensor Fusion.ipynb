{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import important libraries here\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Vardhan Mistry/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [r'C:\\Users\\Admin\\Desktop\\Projects\\sensor_fusion\\dataset_astyx_hires2019\\dataset_astyx_hires2019\\camera_front\\000131.jpg']  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Show detected objects\n",
    "results.show()\n",
    "results.xyxy[0]  # img1 predictions (tensor)\n",
    "df = results.pandas().xyxy[0]\n",
    "\n",
    "# Define output path\n",
    "output_path = r'C:\\Users\\Admin\\Desktop\\Projects\\sensor_fusion\\dataset_astyx_hires2019\\dataset_astyx_hires2019\\BB_Coordinates\\000131.csv'\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calib_astyx():\n",
    "    def __init__(self, file):\n",
    "        # Initialize the calibration object by loading sensor calibration data from a JSON file.\n",
    "        # Parameters:\n",
    "        # - file: A string path to the JSON file containing calibration data for radar, lidar, and camera sensors.\n",
    "\n",
    "        # Load calibration data from a JSON file\n",
    "        with open(file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        # Calibration matrices for converting radar, lidar, and camera data to a reference coordinate system\n",
    "        self.radar2ref = np.array(data[\"sensors\"][0][\"calib_data\"][\"T_to_ref_COS\"])  # Radar to reference\n",
    "        self.lidar2ref_cos = np.array(data[\"sensors\"][1][\"calib_data\"][\"T_to_ref_COS\"])  # Lidar to reference (COS means Coordinate System)\n",
    "        self.camera2ref = np.array(data[\"sensors\"][2][\"calib_data\"][\"T_to_ref_COS\"])  # Camera to reference\n",
    "        self.K = np.array(data[\"sensors\"][2][\"calib_data\"][\"K\"])  # Intrinsic camera matrix\n",
    "        \n",
    "        # Compute inverse transformations for mapping from the reference coordinate system back to sensor-specific coordinate systems\n",
    "        self.ref2radar = self.inv_trans(self.radar2ref)\n",
    "        self.ref2lidar = self.inv_trans(self.lidar2ref_cos)\n",
    "        self.ref2camera = self.inv_trans(self.camera2ref)\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_trans(T):\n",
    "        # Compute the inverse transformation matrix for a given sensor to reference coordinate transformation.\n",
    "        # Parameters:\n",
    "        # - T: A numpy array representing the transformation matrix from sensor to reference coordinates.\n",
    "        # Returns:\n",
    "        # - The inverse transformation matrix as a numpy array, which can be used to map points from the\n",
    "        # reference coordinate system back to the sensor-specific coordinate system.\n",
    "\n",
    "        rotation = np.linalg.inv(T[0:3, 0:3])  # Invert the rotation part\n",
    "        translation = T[0:3, 3]\n",
    "        translation = -1 * np.dot(rotation, translation.T)  # Invert the translation part\n",
    "        translation = np.reshape(translation, (3, 1))\n",
    "        Q = np.hstack((rotation, translation))  # Reassemble the inverted transformation matrix\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def lidar2ref(self, points):\n",
    "        # Convert lidar points from the lidar coordinate system to the reference coordinate system.\n",
    "        # Parameters:\n",
    "        # - points: A numpy array of points in the lidar coordinate system.\n",
    "        # Returns:\n",
    "        # - A numpy array of the same points transformed to the reference coordinate system.\n",
    "\n",
    "        n = points.shape[0]\n",
    "        \n",
    "        points_hom = np.hstack((points, np.ones((n,1))))  # Convert points to homogeneous coordinates\n",
    "        points_ref = np.dot(points_hom, np.transpose(self.lidar2ref_cos))  # Transform points to reference coordinate system\n",
    "        \n",
    "        return points_ref[:,0:3]  # Return the transformed points, discarding the homogeneous coordinate\n",
    "\n",
    "    def ref2Camera(self, points, img_size):\n",
    "        # Project points from the reference coordinate system onto the camera image plane.\n",
    "        # Parameters:\n",
    "        # - points: A numpy array of points in the reference coordinate system.\n",
    "        # - img_size: A tuple representing the size of the camera image (width, height).\n",
    "        # Returns:\n",
    "        # - A tuple containing a numpy array of the projected points on the camera image plane and a mask\n",
    "        # array indicating which points are within the image frame.\n",
    "\n",
    "        obj_image = np.dot(self.ref2camera[0:3, 0:3], points.T)  # Apply rotation\n",
    "        T = self.ref2camera[0:3, 3]\n",
    "        obj_image = obj_image + T[:, np.newaxis]  # Apply translation\n",
    "        obj_image = np.dot(self.K, obj_image)  # Apply intrinsic camera matrix\n",
    "        obj_image = obj_image / obj_image[2]  # Normalize by the third row to project onto image plane\n",
    "        obj_image = np.delete(obj_image, 2, 0)  # Remove the third row\n",
    "        \n",
    "        # Create a mask to filter out points that are outside the image frame or behind the camera\n",
    "        mask = (obj_image[0,:] <= img_size[0]) & \\\n",
    "               (obj_image[1,:] <= img_size[1]) & \\\n",
    "               (obj_image[0,:] >= 0) & (obj_image[1,:] >= 0) & \\\n",
    "               (points[:,0] >= 0)\n",
    "        return obj_image, mask  # Return the projected points and the mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Object3d(object):\n",
    "    \"\"\"\n",
    "    Represents a 3D object with attributes extracted from a given input dictionary.\n",
    "    This class handles the extraction and manipulation of various object properties,\n",
    "    including its orientation, bounding box, and angle calculations based on quaternion\n",
    "    orientation data.\n",
    "\n",
    "    Attributes:\n",
    "    - type (str): The class name of the object.\n",
    "    - occlusion (float): The occlusion level of the object.\n",
    "    - quat (numpy.ndarray): Quaternion representing the object's orientation.\n",
    "    - rotationmatrix (numpy.ndarray): Rotation matrix derived from the quaternion.\n",
    "    - h (float): Height of the object's bounding box.\n",
    "    - w (float): Width of the object's bounding box.\n",
    "    - l (float): Length of the object's bounding box.\n",
    "    - t (tuple): A tuple representing the center of the object's bounding box.\n",
    "    - distance (float): Distance of the object from the origin, calculated using its bounding box center.\n",
    "    - bbox (numpy.ndarray): Coordinates of the object's bounding box corners.\n",
    "    - angle (float): Angle of the object's orientation in degrees.\n",
    "    \"\"\"\n",
    "    def __init__(self, obj):\n",
    "        \"\"\"\n",
    "        Initializes an Object3d instance by extracting relevant attributes from the input dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        - obj (dict): A dictionary containing object properties, including its class name,\n",
    "                      dimensions, center, occlusion level, and orientation quaternion.\n",
    "        \"\"\"\n",
    "        # Extract basic attributes\n",
    "        self.type = obj[\"classname\"]\n",
    "        self.occlusion = obj[\"occlusion\"] \n",
    "        self.quat = np.array(obj[\"orientation_quat\"])\n",
    "        self.rotationmatrix = self.get_rotationmatrix()\n",
    "        \n",
    "        # Extract 3D bounding box information\n",
    "        self.h = obj[\"dimension3d\"][2]  # box height\n",
    "        self.w = obj[\"dimension3d\"][1]  # box width\n",
    "        self.l = obj[\"dimension3d\"][0]  # box length (in meters)\n",
    "        self.t = (obj[\"center3d\"][0], obj[\"center3d\"][1], obj[\"center3d\"][2])\n",
    "        \n",
    "        self.distance = np.sqrt(np.sum(np.square(self.t)))\n",
    "        \n",
    "        # Calculate bounding box corners and object's angle\n",
    "        self.bbox = self.get_bbox()\n",
    "        self.angle = self.get_angle()[2]\n",
    "        if self.angle >= 0:\n",
    "            self.angle = self.angle * 180 / np.pi\n",
    "        else:\n",
    "            self.angle = self.angle * 180 / np.pi + 360\n",
    "\n",
    "    def get_angle(self):\n",
    "        \"\"\"\n",
    "        Calculates the Euler angles from the object's orientation quaternion.\n",
    "\n",
    "        Returns:\n",
    "        - A tuple of Euler angles (roll, pitch, yaw) in radians.\n",
    "        \"\"\"\n",
    "        w, x, y, z = self.quat\n",
    "        return (math.atan2(2*(w*x+y*z), 1-2*(x*x+y*y)),\n",
    "                math.asin(2*(w*y-z*x)),\n",
    "                math.atan2(2*(w*z+x*y), 1-2*(y*y+z*z)))\n",
    "\n",
    "    def get_bbox(self):\n",
    "        \"\"\"\n",
    "        Calculates the coordinates of the object's bounding box corners based on its\n",
    "        center, dimensions, and orientation.\n",
    "\n",
    "        Returns:\n",
    "        - A numpy.ndarray containing the coordinates of the bounding box corners.\n",
    "        \"\"\"\n",
    "        center = np.array(self.t)\n",
    "        dimension = np.array([self.l, self.w, self.h])\n",
    "\n",
    "        # Adjust dimensions for bounding box calculation\n",
    "        w, l, h = dimension[0] + 0.1, dimension[1] + 0.1, dimension[2]\n",
    "\n",
    "        # Define corner points in local object coordinates\n",
    "        x_corners = [-w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2, w / 2]\n",
    "        y_corners = [l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2]\n",
    "        z_corners = [h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2, h / 2]\n",
    "\n",
    "        # Apply rotation and translate to global coordinates\n",
    "        R = self.rotationmatrix\n",
    "        bbox = np.vstack([x_corners, y_corners, z_corners])\n",
    "        bbox = np.dot(R, bbox)\n",
    "        bbox = bbox + center[:, np.newaxis]\n",
    "        bbox = np.transpose(bbox)\n",
    "\n",
    "        return bbox\n",
    "    \n",
    "    def get_rotationmatrix(self):\n",
    "        \"\"\"\n",
    "        Converts the object's orientation quaternion into a rotation matrix.\n",
    "\n",
    "        Returns:\n",
    "        - A numpy.ndarray representing the rotation matrix.\n",
    "        \"\"\"\n",
    "        n = np.dot(self.quat, self.quat)\n",
    "        if n < np.finfo(self.quat.dtype).eps:\n",
    "            return np.identity(4)  # Return identity matrix if quaternion is near zero\n",
    "        \n",
    "        q = self.quat * np.sqrt(2.0 / n)\n",
    "        q = np.outer(q, q)\n",
    "        rot_matrix = np.array(\n",
    "            [[1.0 - q[2, 2] - q[3, 3], q[1, 2] + q[3, 0], q[1, 3] - q[2, 0]],\n",
    "             [q[1, 2] - q[3, 0], 1.0 - q[1, 1] - q[3, 3], q[2, 3] + q[1, 0]],\n",
    "             [q[1, 3] + q[2, 0], q[2, 3] - q[1, 0], 1.0 - q[1, 1] - q[2, 2]]],\n",
    "            dtype=q.dtype)\n",
    "        rot_matrix = np.transpose(rot_matrix)\n",
    "        return rot_matrix\n",
    "\n",
    "class calib_astyx():\n",
    "    \"\"\"\n",
    "    This class is designed to manage the calibration of sensor data in autonomous vehicle contexts.\n",
    "    \n",
    "    It supports the conversion of sensor data between different coordinate systems, specifically for radar,\n",
    "    lidar, and camera sensors, to a common reference coordinate system and vice versa. It also handles the\n",
    "    projection of points from the reference coordinate system onto the camera image plane.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        Initialize the calibration object by loading sensor calibration data from a JSON file.\n",
    "        \n",
    "        Parameters:\n",
    "        - file: A string path to the JSON file containing calibration data for radar, lidar, and camera sensors.\n",
    "        \n",
    "        The method extracts transformation matrices for each sensor to the reference coordinate system and\n",
    "        the intrinsic camera matrix.\n",
    "        \"\"\"\n",
    "        # Load calibration data from a JSON file\n",
    "        with open(file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        # Calibration matrices for converting radar, lidar, and camera data to a reference coordinate system\n",
    "        self.radar2ref = np.array(data[\"sensors\"][0][\"calib_data\"][\"T_to_ref_COS\"])  # Radar to reference\n",
    "        self.lidar2ref_cos = np.array(data[\"sensors\"][1][\"calib_data\"][\"T_to_ref_COS\"])  # Lidar to reference (COS means Coordinate System)\n",
    "        self.camera2ref = np.array(data[\"sensors\"][2][\"calib_data\"][\"T_to_ref_COS\"])  # Camera to reference\n",
    "        self.K = np.array(data[\"sensors\"][2][\"calib_data\"][\"K\"])  # Intrinsic camera matrix\n",
    "        \n",
    "        # Compute inverse transformations for mapping from the reference coordinate system back to sensor-specific coordinate systems\n",
    "        self.ref2radar = self.inv_trans(self.radar2ref)\n",
    "        self.ref2lidar = self.inv_trans(self.lidar2ref_cos)\n",
    "        self.ref2camera = self.inv_trans(self.camera2ref)\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_trans(T):\n",
    "        \"\"\"\n",
    "        Compute the inverse transformation matrix for a given sensor to reference coordinate transformation.\n",
    "        \n",
    "        Parameters:\n",
    "        - T: A numpy array representing the transformation matrix from sensor to reference coordinates.\n",
    "        \n",
    "        Returns:\n",
    "        - The inverse transformation matrix as a numpy array, which can be used to map points from the\n",
    "          reference coordinate system back to the sensor-specific coordinate system.\n",
    "        \"\"\"\n",
    "        rotation = np.linalg.inv(T[0:3, 0:3])  # Invert the rotation part\n",
    "        translation = T[0:3, 3]\n",
    "        translation = -1 * np.dot(rotation, translation.T)  # Invert the translation part\n",
    "        translation = np.reshape(translation, (3, 1))\n",
    "        Q = np.hstack((rotation, translation))  # Reassemble the inverted transformation matrix\n",
    "\n",
    "        return Q\n",
    "    \n",
    "    def lidar2ref(self, points):\n",
    "        \"\"\"\n",
    "        Convert lidar points from the lidar coordinate system to the reference coordinate system.\n",
    "        \n",
    "        Parameters:\n",
    "        - points: A numpy array of points in the lidar coordinate system.\n",
    "        \n",
    "        Returns:\n",
    "        - A numpy array of the same points transformed to the reference coordinate system.\n",
    "        \"\"\"\n",
    "        n = points.shape[0]\n",
    "        \n",
    "        points_hom = np.hstack((points, np.ones((n,1))))  # Convert points to homogeneous coordinates\n",
    "        points_ref = np.dot(points_hom, np.transpose(self.lidar2ref_cos))  # Transform points to reference coordinate system\n",
    "        \n",
    "        return points_ref[:,0:3]  # Return the transformed points, discarding the homogeneous coordinate\n",
    "\n",
    "    def ref2Camera(self, points, img_size):\n",
    "        \"\"\"\n",
    "        Project points from the reference coordinate system onto the camera image plane.\n",
    "        \n",
    "        Parameters:\n",
    "        - points: A numpy array of points in the reference coordinate system.\n",
    "        - img_size: A tuple representing the size of the camera image (width, height).\n",
    "        \n",
    "        Returns:\n",
    "        - A numpy array of projected points on the camera image plane.\n",
    "        - A boolean mask indicating whether each point is visible in the camera image.\n",
    "        \"\"\"\n",
    "        n = points.shape[0]\n",
    "        \n",
    "        # Convert points to homogeneous coordinates\n",
    "        points_hom = np.hstack((points, np.ones((n,1))))\n",
    "        \n",
    "        # Transform points to camera coordinate system\n",
    "        points_camera = np.dot(points_hom, np.transpose(self.ref2camera))\n",
    "        \n",
    "        # Project points onto camera image plane\n",
    "        projected_points = np.dot(points_camera, np.transpose(self.K))\n",
    "        projected_points = projected_points[:, 0:2] / projected_points[:, 2][:, None]\n",
    "        \n",
    "        # Create mask to filter out points outside the image boundaries\n",
    "        mask = (projected_points[:, 0] >= 0) & (projected_points[:, 0] < img_size[0]) & \\\n",
    "               (projected_points[:, 1] >= 0) & (projected_points[:, 1] < img_size[1])\n",
    "        \n",
    "        return projected_points, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration data\n",
    "calib = calib_astyx(r\"C:\\Users\\Admin\\Desktop\\Projects\\sensor_fusion\\dataset_astyx_hires2019\\dataset_astyx_hires2019\\calibration\\000131.json\")\n",
    "\n",
    "# Load lidar data from text file\n",
    "lidar = np.loadtxt(r\"C:\\Users\\Admin\\Desktop\\Projects\\sensor_fusion\\dataset_astyx_hires2019\\dataset_astyx_hires2019\\lidar_vlp16\\000131.txt\", skiprows=1)\n",
    "\n",
    "lidar_points = lidar[:, :3]  # Selecting only the first three points\n",
    "lidar_points_ref = calib.lidar2ref(lidar_points)\n",
    "\n",
    "\n",
    "# Load camera image using OpenCV\n",
    "img = cv2.imread(r'C:\\Users\\Admin\\Desktop\\Projects\\sensor_fusion\\dataset_astyx_hires2019\\dataset_astyx_hires2019\\camera_front\\000131.jpg')  \n",
    "\n",
    "# Get the height and width of the image\n",
    "img_height, img_width = img.shape[:2]\n",
    "\n",
    "# Project lidar points onto camera image\n",
    "projected_points, mask = calib.ref2Camera(lidar_points_ref, (img_width, img_height))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert projected points to integers for pixel coordinates\n",
    "projected_points_int = projected_points.astype(int)\n",
    "\n",
    "# Draw projected lidar points on the image\n",
    "# Draw projected lidar points on the image\n",
    "for point in projected_points_int:\n",
    "    cv2.circle(img, (point[0], point[1]), radius=3, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "\n",
    "# Show the image with projected lidar points\n",
    "cv2.imshow(\"Projected Lidar Points\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
